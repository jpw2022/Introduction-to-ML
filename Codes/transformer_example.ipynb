{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "SimpleTransformer(\n",
      "  (input_fc): Linear(in_features=10, out_features=64, bias=True)\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
      "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JPW\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Train Loss: 0.7312, Train Accuracy: 49.60%\n",
      "Validation Loss: 0.7037, Validation Accuracy: 49.80%\n",
      "Epoch [2/10]\n",
      "Train Loss: 0.6977, Train Accuracy: 50.80%\n",
      "Validation Loss: 0.7031, Validation Accuracy: 50.20%\n",
      "Epoch [3/10]\n",
      "Train Loss: 0.6993, Train Accuracy: 48.30%\n",
      "Validation Loss: 0.6939, Validation Accuracy: 50.20%\n",
      "Epoch [4/10]\n",
      "Train Loss: 0.7016, Train Accuracy: 48.40%\n",
      "Validation Loss: 0.7001, Validation Accuracy: 50.20%\n",
      "Epoch [5/10]\n",
      "Train Loss: 0.7064, Train Accuracy: 47.50%\n",
      "Validation Loss: 0.6975, Validation Accuracy: 49.80%\n",
      "Epoch [6/10]\n",
      "Train Loss: 0.6957, Train Accuracy: 49.00%\n",
      "Validation Loss: 0.6943, Validation Accuracy: 49.80%\n",
      "Epoch [7/10]\n",
      "Train Loss: 0.6990, Train Accuracy: 49.60%\n",
      "Validation Loss: 0.6945, Validation Accuracy: 49.80%\n",
      "Epoch [8/10]\n",
      "Train Loss: 0.7011, Train Accuracy: 51.60%\n",
      "Validation Loss: 0.6932, Validation Accuracy: 50.20%\n",
      "Epoch [9/10]\n",
      "Train Loss: 0.7009, Train Accuracy: 49.40%\n",
      "Validation Loss: 0.6932, Validation Accuracy: 50.20%\n",
      "Epoch [10/10]\n",
      "Train Loss: 0.6993, Train Accuracy: 48.40%\n",
      "Validation Loss: 0.6937, Validation Accuracy: 50.20%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 定义 Transformer 模型\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        # 线性层将输入从 input_dim 映射到 model_dim\n",
    "        self.input_fc = nn.Linear(input_dim, model_dim)\n",
    "        \n",
    "        # Transformer 编码器部分\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,  # 模型的维度\n",
    "            nhead=num_heads,    # 自注意力机制中的头数\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers  # 编码器层数\n",
    "        )\n",
    "\n",
    "        # 线性层用于最终输出\n",
    "        self.fc_out = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入 x 的形状是 (batch_size, seq_length, input_dim)\n",
    "        # 先通过线性层将输入从 input_dim 映射到 model_dim\n",
    "        x = self.input_fc(x)  # 现在的 x 形状是 (batch_size, seq_length, model_dim)\n",
    "\n",
    "        # 将输入转换为 (seq_length, batch_size, model_dim) 形式\n",
    "        x = x.permute(1, 0, 2)  # 转换为 (seq_length, batch_size, model_dim)\n",
    "\n",
    "        # 使用 Transformer 编码器处理输入\n",
    "        transformer_out = self.transformer_encoder(x)\n",
    "\n",
    "        # 从 Transformer 输出中提取最后一个时刻的隐藏状态\n",
    "        output = transformer_out[-1, :, :]  # 取最后一个时间步，形状 (batch_size, model_dim)\n",
    "\n",
    "        # 通过线性层输出最终的预测结果\n",
    "        output = self.fc_out(output)  # 形状为 (batch_size, output_dim)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# 配置模型参数\n",
    "input_dim = 10  # one-hot 编码的维度，假设有 10 个类\n",
    "model_dim = 64  # Transformer 中每层的维度\n",
    "num_heads = 8   # 自注意力机制的头数\n",
    "num_layers = 4  # Transformer 编码器的层数\n",
    "output_dim = 2  # 二分类问题\n",
    "\n",
    "# 检查是否有可用的 GPU，如果有则使用 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 创建模型实例\n",
    "model = SimpleTransformer(input_dim, model_dim, num_heads, num_layers, output_dim).to(device)\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)\n",
    "\n",
    "# 假设的训练数据\n",
    "batch_size = 32\n",
    "seq_length = 50  # 假设输入序列长度为 50\n",
    "num_samples = 1000  # 假设我们有 1000 个样本\n",
    "\n",
    "# 随机生成一批训练数据\n",
    "x_train = torch.randn(num_samples, seq_length, input_dim).to(device)  # 移动数据到 GPU\n",
    "y_train = torch.randint(0, 2, (num_samples,)).to(device)  # 移动标签到 GPU\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 随机生成一些验证数据\n",
    "x_val = torch.randn(num_samples, seq_length, input_dim).to(device)  # 验证数据\n",
    "y_val = torch.randint(0, 2, (num_samples,)).to(device)  # 验证标签\n",
    "\n",
    "# 创建验证数据集和数据加载器\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()  # 二分类任务使用交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 定义训练过程\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # 将每个批次的数据和标签移到 GPU\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)  # 输出形状 (batch_size, output_dim)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, labels)  # 交叉熵损失\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()  # 清空之前的梯度\n",
    "        loss.backward()  # 计算梯度\n",
    "        optimizer.step()  # 更新参数\n",
    "\n",
    "        # 计算准确率\n",
    "        _, predicted = torch.max(outputs, 1)  # 获取最大概率的类\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = correct / total * 100\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 定义验证过程\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # 在评估时不计算梯度\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 计算准确率\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    accuracy = correct / total * 100\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 训练和验证循环\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练\n",
    "    train_loss, train_accuracy = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "    # 验证\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
